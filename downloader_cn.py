# -*- coding: utf-8 -*-
import os, sys, time, random, json, subprocess
import pandas as pd
import yfinance as yf
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ========== åƒæ•¸èˆ‡è·¯å¾‘è¨­å®š ==========
MARKET_CODE = "cn-share"
DATA_SUBDIR = "dayK"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, DATA_SUBDIR)
LIST_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, "lists")
CACHE_LIST_PATH = os.path.join(LIST_DIR, "cn_stock_list_cache.json")

# ğŸš€ ç¨å¾®æå‡ä¸¦è¡Œæ•¸ï¼Œ8 æ˜¯ GitHub Actions ç©©å®šçš„ä¸Šé™
THREADS_CN = 8 
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(LIST_DIR, exist_ok=True)

def log(msg: str):
    print(f"{pd.Timestamp.now():%H:%M:%S}: {msg}")

def ensure_pkg(pkg: str):
    try:
        __import__(pkg)
    except ImportError:
        log(f"ğŸ”§ æ­£åœ¨å®‰è£ {pkg}...")
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", pkg])

def get_cn_list():
    """ç²å– A è‚¡æ¸…å–®ï¼šæ•´åˆ EM æ¥å£èˆ‡å¤šé‡ä¿åº•æ©Ÿåˆ¶"""
    ensure_pkg("akshare")
    import akshare as ak
    threshold = 4500  
    
    if os.path.exists(CACHE_LIST_PATH):
        try:
            file_mtime = os.path.getmtime(CACHE_LIST_PATH)
            if datetime.fromtimestamp(file_mtime).date() == datetime.now().date():
                with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if len(data) >= threshold:
                        log(f"ğŸ“¦ è¼‰å…¥ä»Šæ—¥å¿«å– (å…± {len(data)} æª”)")
                        return data
        except Exception as e:
            log(f"âš ï¸ å¿«å–è®€å–å¤±æ•—: {e}")

    log("ğŸ“¡ å˜—è©¦å¾ Akshare EM æ¥å£ç²å–æ¸…å–®...")
    try:
        df_sh = ak.stock_sh_a_spot_em()
        df_sz = ak.stock_sz_a_spot_em()
        df = pd.concat([df_sh, df_sz], ignore_index=True)
        
        df['code'] = df['ä»£ç '].astype(str).str.zfill(6)
        valid_prefixes = ('000','001','002','003','300','301','600','601','603','605','688')
        df = df[df['code'].str.startswith(valid_prefixes)]
        
        res = [f"{row['code']}&{row['åç§°']}" for _, row in df.iterrows()]
        
        if len(res) >= threshold:
            with open(CACHE_LIST_PATH, "w", encoding="utf-8") as f:
                json.dump(res, f, ensure_ascii=False)
            log(f"âœ… æˆåŠŸç²å– {len(res)} æª”æ¨™çš„")
            return res
    except Exception as e:
        log(f"âš ï¸ EM æ¥å£å¤±æ•—: {e}")

    if os.path.exists(CACHE_LIST_PATH):
        log("ğŸ”„ æ¥å£å…¨æ•¸å¤±æ•—ï¼Œä½¿ç”¨æ­·å²å¿«å–å‚™æ´...")
        with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
            return json.load(f)

    return ["600519&è²´å·èŒ…å°", "000001&å¹³å®‰éŠ€è¡Œ", "300750&å¯§å¾·æ™‚ä»£", "601318&ä¸­åœ‹å¹³å®‰"]

def download_one(item):
    """å–®æª”ä¸‹è¼‰é‚è¼¯ï¼šå„ªåŒ–éš¨æ©Ÿå»¶é²ä»¥ç¸®çŸ­ç¸½è€—æ™‚"""
    code, name = item.split('&', 1)
    symbol = f"{code}.SS" if code.startswith('6') else f"{code}.SZ"
    out_path = os.path.join(DATA_DIR, f"{code}_{name}.csv")

    # ğŸš€ å¼·åŒ–çºŒè·‘åˆ¤æ–·ï¼Œè‹¥æª”æ¡ˆå­˜åœ¨ä¸”æœ‰å…§å®¹å‰‡è·³é
    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:
        return {"status": "exists", "code": code}

    max_retries = 2
    for attempt in range(max_retries):
        try:
            # ğŸš€ ç¸®çŸ­éš¨æ©Ÿç­‰å¾…æ™‚é–“ï¼Œå¤§å¹…æå‡æ•´é«”é€Ÿåº¦
            time.sleep(random.uniform(0.4, 1.0)) 
            
            tk = yf.Ticker(symbol)
            hist = tk.history(period="2y", timeout=20)
            
            if hist is not None and not hist.empty:
                hist.reset_index(inplace=True)
                hist.columns = [c.lower() for c in hist.columns]
                if 'date' in hist.columns:
                    hist['date'] = pd.to_datetime(hist['date'], utc=True).dt.tz_localize(None)
                
                hist.to_csv(out_path, index=False, encoding='utf-8-sig')
                return {"status": "success", "code": code}
            else:
                if attempt == max_retries - 1:
                    return {"status": "empty", "code": code}
                
        except Exception:
            if attempt == max_retries - 1:
                return {"status": "error", "code": code}
            time.sleep(2) 
            
    return {"status": "error", "code": code}

def main():
    start_time = time.time()
    log("ğŸ‡¨ğŸ‡³ ä¸­åœ‹ A è‚¡æ•¸æ“šåŒæ­¥å™¨å•Ÿå‹• (ä¸¦è¡Œåº¦å„ªåŒ–ç‰ˆ)")
    
    items = get_cn_list()
    log(f"ğŸš€ ç›®æ¨™ç¸½æ•¸: {len(items)} æª”")
    
    stats = {"success": 0, "exists": 0, "empty": 0, "error": 0}
    
    with ThreadPoolExecutor(max_workers=THREADS_CN) as executor:
        futures = {executor.submit(download_one, it): it for it in items}
        pbar = tqdm(total=len(items), desc="ä¸‹è¼‰é€²åº¦")
        
        for f in as_completed(futures):
            res = f.result()
            stats[res.get("status", "error")] += 1
            pbar.update(1)
        
        pbar.close()

    # ğŸš€ æº–å‚™çµ±è¨ˆæ•¸æ“šä¾›å›å‚³
    total_expected = len(items)
    effective_success = stats['success'] + stats['exists']
    fail_count = stats['error'] + stats['empty']

    download_stats = {
        "total": total_expected,
        "success": effective_success,
        "fail": fail_count
    }

    duration = (time.time() - start_time) / 60
    log(f"ğŸ“Š åŸ·è¡Œå ±å‘Š (è€—æ™‚ {duration:.1f} åˆ†é˜):")
    log(f"   - æ‡‰æ”¶ç¸½æ•¸: {total_expected}")
    log(f"   - æˆåŠŸ(å«èˆŠæª”): {effective_success}")
    log(f"   - å¤±æ•—/ç„¡æ•¸æ“š: {fail_count}")
    log(f"ğŸ“ˆ æ•¸æ“šå®Œæ•´åº¦: {(effective_success/total_expected)*100:.2f}%")
    
    return download_stats # ğŸš€ ç¢ºä¿ main() å›å‚³çµ±è¨ˆï¼Œä¾› notifier ä½¿ç”¨

if __name__ == "__main__":
    main()
